network:
  - '256-1-n-lin-concat'             # <next layer dim>-<order>-<bias>-<norm>-<act>    order: 0,1,2    bias: b,nb      act: relu,lin   norm: n,nn
  - '128-0-n-relu-mean'
  - '128-1-n-relu-concat'
  - '128-0-n-relu-mean'
  - '128-1-n-relu-concat'
  - '128-0-n-relu-concat'
  - 'softmax'               # output layer: sigmoid for multi-class; softmax for single class
params:
  - lr: 0.01
    weight_decay: 0.0       # 001
    norm_weight: 1
    norm_adj: 'rw'
    model: 'gsaint'     # 'gs_mean' 'gsaint'
    q_threshold: 50
    q_offset: 0
    batch_norm: 'tf.nn'
    skip: 'noskip'
phase:
  - end: 50                 # end epoch for this phase
    dropout: 0.0
    sampler: 'frontier'
    size_subgraph: 8000
    size_frontier: 1000
    order: 1
    max_deg: 10000


# check if the layer should be n or b
# good yml targeting at 96.4 -- less than 30 epochs
# # this is using the norm of nn.batch_norm
