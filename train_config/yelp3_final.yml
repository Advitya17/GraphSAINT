network:
  - '512-0-n-relu-concat'
  - '256-1-n-relu-concat'             # hidden layer dimension (before concat)
  - '256-1-n-relu-concat'
  # - '512-0-n-relu-concat'
  - '256-2-n-relu-concat'
  # - '512-0-n-relu-concat'
  # - '512-0-n-relu-concat'
  - 'sigmoid'       # output layer: sigmoid for multi-class; softmax for single class
params:
  - lr: 0.03
    weight_decay: 0.0 #001
    norm_weight: 1
    norm_adj: 'sym'
    model: 'gsaint'     # 'gs_mean' 'gsaint'
    q_threshold: 50
    q_offset: 5
    batch_norm: 'tf.layers'
    skip: '0-2' # 'noskip' for no skip
phase:
  #- end: 5
  #  dropout: 0.0
  #  sampler: 'frontier'
  #  size_subgraph: 3000
  #  size_frontier: 1000
  #  order: 1
  #  max_deg: 50
  - end: 30         # end epoch for this phase
    dropout: 0.05
    sampler: 'frontier'
    size_subgraph: 2000
    size_frontier: 1000
    order: 1
    max_deg: 10000
    # sampler: 'rw'
    # num_root: 500
    # sampler: 'edge'
    # size_subgraph: 2000
    # depth: 4
  # - end: 50         # end epoch for this phase
  #   dropout: 0.3
  #   sampler: 'frontier'
  #   size_subgraph: 2000
  #   size_frontier: 1000
  #   order: 1
  #   max_deg: 10000
