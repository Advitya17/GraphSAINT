# A*W; ReLU-norm
network:
  - dim: 128
    aggr: 'concat'
    loss: 'softmax'
    arch: '1-0-1-0'
    act: 'relu'
    bias: 'norm'
params:
  - lr: 0.01
    weight_decay: 0.0   
    norm_loss: 1
    norm_aggr: 1
    q_threshold: 50
    q_offset: 0
phase:
  - end: 20
    dropout: 0.0
    sampler: 'rw'
    num_root: 2000
    depth: 4
