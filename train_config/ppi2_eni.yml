network:
  ### NOTE ###: an order-1 layer followed by an order-0 layer is equivalent to a single layer in S-GCN (Chen, ICML'18). 
  # - '100-1-n-relu-mean'
  # - '512-0-n-relu-mean'
  # - '1024-1-n-relu-concat'        
  # - '512-0-n-relu-concat'
  - '512-1-n-relu-concat'
  - '512-0-n-relu-concat'
  - '512-1-n-relu-concat'        
  - '512-0-n-relu-concat'
  - 'sigmoid'
params:
  - lr: 0.01
    weight_decay: 0.0
    norm_loss: 0
    norm_aggr: 1
    norm_beta: 0
    norm_layer: 'all'
    split_beta: 1
    norm_adj: 'rw'
    model: 'gsaint'
    q_threshold: 50
    q_offset: 0
    batch_norm: 'tf.layers'
    skip: 'noskip'
    reset_opt: False
phase:
  # We set small subgraph sizes for the initial phases. This speeds up convergence without affecting accuracy.
  - end: 300
    dropout: 0.0
    #sampler: 'frontier'
    #size_subgraph: 5000
    #size_frontier: 1800
    #order: 1
    #max_deg: 10000
    sampler: 'edge_indp'
    size_subg_edge: 2000
    is_induced: 0
    level_approx: 0
